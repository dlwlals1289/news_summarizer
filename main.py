import feedparser
import openai
import os
import time
import re
import html
import requests 
import argparse  # ì»¤ë§¨ë“œë¼ì¸ ì¸ì ì²˜ë¦¬ìš©
from urllib.parse import urlparse
from datetime import datetime, timezone, timedelta
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv
from dateutil import parser as date_parser
import notion_client


load_dotenv()

# í•œêµ­ ì£¼ìš” ì–¸ë¡ ì‚¬ RSS í”¼ë“œ
RSS_FEEDS = {
    "í•œêµ­ê²½ì œ": "https://www.hankyung.com/feed/economy",
    "ë§¤ì¼ê²½ì œ": "https://www.mk.co.kr/rss/30200001/",
    "ì „ìì‹ ë¬¸": "https://www.etnews.com/20/0101/list.xml",
    "ì¡°ì„ ë¹„ì¦ˆ": "https://biz.chosun.com/rss/biz_total.xml",
    "ì„œìš¸ê²½ì œ": "https://www.sedaily.com/RSS/S01.xml",
}

KST = timezone(timedelta(hours=9))

def _clean_html(text: str) -> str:
    """HTML íƒœê·¸/ì—”í‹°í‹° ì œê±°"""
    if not text:
        return ""
    text = html.unescape(text)
    text = re.sub(r"<[^>]+>", "", text)
    return text.strip()

def collect_news_from_rss(target_date=None):
    """RSSë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (fallback / ë˜ëŠ” ê¸°ë³¸)"""
    print("ğŸ“° RSSë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    all_articles = []
    target_day = None

    if target_date:
        try:
            target_day = datetime.strptime(target_date, "%Y-%m-%d").date()
            print(f"ğŸ“… ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: {target_day.strftime('%Yë…„ %mì›” %dì¼')}\n")
        except ValueError:
            print(f"âŒ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {target_date} (YYYY-MM-DD í˜•ì‹ í•„ìš”)")
            return []
    else:
        print("ğŸ“… ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: ì „ì²´(í”¼ë“œ ìµœì‹  ê¸°ì‚¬)\n")

    for source, url in RSS_FEEDS.items():
        try:
            print(f"  â†’ {source} ìˆ˜ì§‘ ì¤‘...")
            feed = feedparser.parse(url)

            count = 0
            for entry in feed.entries[:100]:
                published_raw = entry.get("published", "")

                if target_day is not None:
                    entry_date = None
                    try:
                        if entry.get("published_parsed"):
                            parsed = entry.published_parsed
                            dt_utc = datetime(*parsed[:6], tzinfo=timezone.utc)
                            entry_date = dt_utc.astimezone(KST).date()
                        elif published_raw:
                            dt = date_parser.parse(published_raw)
                            if dt.tzinfo is None:
                                dt = dt.replace(tzinfo=KST)
                            entry_date = dt.astimezone(KST).date()
                    except Exception:
                        entry_date = None

                    if entry_date != target_day:
                        continue

                title = _clean_html(entry.get("title", ""))
                summary = _clean_html(entry.get("summary", ""))[:500]

                article = {
                    "title": title,
                    "link": entry.get("link", ""),
                    "published": published_raw,
                    "summary": summary,
                    "source": source,
                    "originallink": entry.get("link", ""),  # RSSëŠ” ë³´í†µ linkê°€ ì›ë¬¸
                }
                if article["title"]:
                    all_articles.append(article)
                    count += 1

            print(f"     âœ“ {count}ê°œ ìˆ˜ì§‘")
            time.sleep(0.3)

        except Exception as e:
            print(f"     âœ— ì˜¤ë¥˜: {e}")

    print(f"\nì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!\n")
    return all_articles

def collect_news_from_naver(target_date=None):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
    
    Args:
        target_date (str): 'YYYY-MM-DD' í˜•ì‹ ë˜ëŠ” None (ì˜¤ëŠ˜)
    """
    print("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ìˆ˜ì§‘ ì‹œì‘...")
    
    # ë‚ ì§œ ì„¤ì •
    if target_date:
        try:
            target_day = datetime.strptime(target_date, "%Y-%m-%d").date()
            print(f"ğŸ“… ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: {target_day.strftime('%Yë…„ %mì›” %dì¼')}\n")
            day_gap = (datetime.now(KST).date() - target_day).days
            if day_gap > 180:
                print("âš ï¸  ëŒ€ìƒ ë‚ ì§œê°€ ë§ì´ ê³¼ê±°ì…ë‹ˆë‹¤. ë„¤ì´ë²„ ê²€ìƒ‰ API(ìµœëŒ€ 1000ê±´) í•œê³„ë¡œ 0ê±´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
        except ValueError:
            print(f"âŒ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {target_date} (YYYY-MM-DD í˜•ì‹ í•„ìš”)")
            return []
    else:
        target_day = datetime.now(KST).date()
        print(f"ğŸ“… ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: ì˜¤ëŠ˜ ({target_day.strftime('%Yë…„ %mì›” %dì¼')})\n")

    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")

    if not client_id or not client_secret:
        print("âŒ ë„¤ì´ë²„ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. RSSë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n")
        return collect_news_from_rss(target_date=target_date)

    keywords = [
        "ê¸ˆìœµ", "ì¦ì‹œ", "ì£¼ì‹", "í™˜ìœ¨", "ì¦ê¶Œ", "ìºí”¼íƒˆ", 
        "IT", "AI", "í…Œí¬", "ìŠ¤í…Œì´ë¸”ì½”ì¸", "ë””ì§€í„¸ìì‚°",
        "ì‚¼ì„±ì¦ê¶Œ", "ë„¤ì´ë²„", 
        "í•˜ë‚˜ì€í–‰", "ìš°ë¦¬ì€í–‰", "ì€í–‰", "ê¸°ì—…ì€í–‰",
    ]

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }

    def _to_kst_date(pub_date_str):
        if not pub_date_str:
            return None
        try:
            pub_date = date_parser.parse(pub_date_str)
            if pub_date.tzinfo is None:
                pub_date = pub_date.replace(tzinfo=KST)
            return pub_date.astimezone(KST).date()
        except Exception:
            return None

    all_articles = []
    display = 100
    max_start = 1000  # ë„¤ì´ë²„ ê²€ìƒ‰ API start íŒŒë¼ë¯¸í„° ìƒí•œ

    for keyword in keywords:
        try:
            print(f"  â†’ '{keyword}' ê²€ìƒ‰ ì¤‘...")
            count = 0
            start = 1
            reached_api_limit_without_target = False

            while start <= max_start:
                params = {
                    "query": keyword,
                    "display": display,
                    "sort": "date",
                    "start": start,
                }
                response = requests.get(url, headers=headers, params=params, timeout=10)

                if response.status_code != 200:
                    print(f"     âœ— ì˜¤ë¥˜: {response.status_code} (start={start})")
                    break

                data = response.json()
                items = data.get("items", [])
                if not items:
                    break

                page_oldest_day = None
                for item in items:
                    pub_date_str = item.get("pubDate", "")
                    pub_day = _to_kst_date(pub_date_str)
                    if pub_day is None:
                        continue

                    if page_oldest_day is None or pub_day < page_oldest_day:
                        page_oldest_day = pub_day

                    # ì§€ì •ëœ ë‚ ì§œê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ (KST ê¸°ì¤€ ì¼ì ë¹„êµ)
                    if pub_day != target_day:
                        continue

                    title = _clean_html(item.get("title", ""))
                    description = _clean_html(item.get("description", ""))[:500]

                    originallink = item.get("originallink") or ""
                    link = item.get("link") or originallink

                    source_domain = "ë„¤ì´ë²„ë‰´ìŠ¤"
                    try:
                        if originallink:
                            source_domain = urlparse(originallink).netloc or "ë„¤ì´ë²„ë‰´ìŠ¤"
                    except Exception:
                        pass

                    article = {
                        "title": title,
                        "link": link,
                        "published": pub_date_str,
                        "summary": description,
                        "source": source_domain,
                        "originallink": originallink,
                    }

                    if article["title"]:
                        all_articles.append(article)
                        count += 1

                # ì˜¤ëŠ˜ ìˆ˜ì§‘ ëª¨ë“œëŠ” ì²« í˜ì´ì§€ë§Œ ì¡°íšŒ
                if target_date is None:
                    break

                # ê²°ê³¼ê°€ ë‚ ì§œ ë‚´ë¦¼ì°¨ìˆœì´ë¯€ë¡œ, í˜ì´ì§€ ìµœì†Ÿê°’ì´ target_dayë³´ë‹¤ ì‘ì•„ì§€ë©´ ì¢…ë£Œ
                if page_oldest_day and page_oldest_day < target_day:
                    break

                if start == max_start and page_oldest_day and page_oldest_day > target_day and count == 0:
                    reached_api_limit_without_target = True

                start += display
                time.sleep(0.12)

            print(f"     âœ“ {count}ê°œ ìˆ˜ì§‘")
            if target_date and count == 0 and reached_api_limit_without_target:
                print("     âš ï¸  API ìµœëŒ€ 1000ê±´ ë²”ìœ„ì—ì„œ í•´ë‹¹ ë‚ ì§œê¹Œì§€ ë‚´ë ¤ê°€ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            time.sleep(0.12)

        except Exception as e:
            print(f"     âœ— ì˜¤ë¥˜: {e}")

    if target_date and not all_articles:
        print("âš ï¸  ë„¤ì´ë²„ì—ì„œ ëŒ€ìƒ ë‚ ì§œ ê¸°ì‚¬ 0ê±´ì…ë‹ˆë‹¤. RSS í´ë°±ì„ ì‹œë„í•©ë‹ˆë‹¤.\n")
        rss_articles = collect_news_from_rss(target_date=target_date)
        if rss_articles:
            return rss_articles

    print(f"\nì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!\n")
    return all_articles


def dedup_by_url(articles):
    """URL ê¸°ì¤€ 1ì°¨ ì¤‘ë³µ ì œê±° (ë„¤ì´ë²„ í‚¤ì›Œë“œ ë£¨í”„ ì¤‘ë³µ ë°©ì§€)"""
    seen = set()
    out = []
    for a in articles:
        key = (a.get("originallink") or a.get("link") or "").strip()
        if not key:
            # urlì´ ì—†ìœ¼ë©´ ì œëª© ê¸°ë°˜ìœ¼ë¡œë¼ë„ í‚¤ ìƒì„±
            key = f"title::{a.get('title','')}"
        if key in seen:
            continue
        seen.add(key)
        out.append(a)
    return out

def remove_duplicates_tfidf(articles, threshold=0.72):
    """TF-IDF ê¸°ë°˜ 2ì°¨ ì¤‘ë³µ ì œê±° (title+summary)"""
    print("ğŸ” ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì¤‘...")

    if not articles:
        return []

    docs = [(a.get("title","") + " " + a.get("summary","")).strip() for a in articles]

    try:
        vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform(docs)
        sim = cosine_similarity(tfidf_matrix)

        keep = []
        removed = set()

        for i in range(len(articles)):
            if i in removed:
                continue
            keep.append(i)
            for j in range(i + 1, len(articles)):
                if sim[i][j] >= threshold:
                    removed.add(j)

        unique = [articles[i] for i in keep]
        print(f"  â†’ {len(removed)}ê°œ ì¤‘ë³µ ì œê±°")
        print(f"  â†’ {len(unique)}ê°œ ê³ ìœ  ê¸°ì‚¬ ë‚¨ìŒ\n")
        return unique

    except Exception as e:
        print(f"  âš ï¸  ì¤‘ë³µ ì œê±° ì˜¤ë¥˜: {e}")
        print(f"  â†’ ì›ë³¸ {len(articles)}ê°œ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n")
        return articles

def summarize_news(articles):
    """AI ìš”ì•½"""
    print("ğŸ¤– OpenAI GPT AI ìš”ì•½ ìƒì„± ì¤‘...\n")

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return "âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    client = openai.OpenAI(api_key=api_key)

    # ìš”ì•½ ì…ë ¥ì„ titleë§Œ ë„£ì§€ ë§ê³  summaryë„ ê°™ì´
    # ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ë¹„ìš©/í† í° ì¦ê°€í•˜ë‹ˆ 80ê°œ ì •ë„ë¡œ ì œí•œ ê¶Œì¥
    selected = articles[:80]
    articles_text = "\n\n".join([
        f"[{a.get('source','')}] {a.get('title','')}\n- ìš”ì•½: {a.get('summary','')}\n- ë§í¬: {a.get('originallink') or a.get('link')}"
        for a in selected
    ])

    prompt = f"""ë‹¤ìŒì€ ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ í•œêµ­ ê²½ì œ/IT ë‰´ìŠ¤ ê¸°ì‚¬ {len(selected)}ê°œì…ë‹ˆë‹¤.

{articles_text}

ìš”êµ¬ì‚¬í•­:
- **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ê¸°ì¤€**:
  - 'ê²½ì œ' ì„¹ì…˜: ì€í–‰, ì¦ê¶Œ, ë³´í—˜, ì¹´ë“œ, ìì‚°ìš´ìš© ë“± ê¸ˆìœµê¸°ê´€ ê´€ë ¨ ì†Œì‹. ê¸°ì—…ì˜ ì‹¤ì  ë°œí‘œ, íˆ¬ì, M&A, ì§€ë¶„ ë³€ë™, ì •ë¶€ì˜ ê²½ì œ ì •ì±… ë“±.
  - 'IT/ê¸°ìˆ ' ì„¹ì…˜: ì¸ê³µì§€ëŠ¥(AI), ì†Œí”„íŠ¸ì›¨ì–´, í•˜ë“œì›¨ì–´, í†µì‹ , ë¸”ë¡ì²´ì¸, í”Œë«í¼ ê¸°ì—…(ë„¤ì´ë²„, ì¹´ì¹´ì˜¤ ë“±) ê´€ë ¨ ì†Œì‹. ê¸°ìˆ  ê°œë°œ, ì‹ ì œí’ˆ ì¶œì‹œ, IT ì„œë¹„ìŠ¤ ì—…ë°ì´íŠ¸ ë“±.
  - ê¸°ì‚¬ ë‚´ìš©ì´ ë‘ ì„¹ì…˜ì— ëª¨ë‘ í•´ë‹¹ë  ê²½ìš°, **ë” í•µì‹¬ì ì¸ ì£¼ì œê°€ ë˜ëŠ” ì„¹ì…˜ì—ë§Œ í¬í•¨**ì‹œí‚¤ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ 'ì€í–‰ì˜ AI ë„ì…'ì€ 'ê²½ì œ' ì„¹ì…˜ì— ë” ê°€ê¹ìŠµë‹ˆë‹¤.
- **ì¤‘ë³µ ì´ìŠˆ í†µí•©**: ë‚´ìš©ì´ ìœ ì‚¬í•˜ê±°ë‚˜ ê°™ì€ ì‚¬ê±´ì„ ë‹¤ë£¨ëŠ” ê¸°ì‚¬ëŠ” **ë°˜ë“œì‹œ í•˜ë‚˜ì˜ í•­ëª©ìœ¼ë¡œ í†µí•©**í•˜ì—¬ ìš”ì•½í•˜ì„¸ìš”. ì ˆëŒ€ë¡œ ê°™ì€ ë‚´ìš©ì˜ ê¸°ì‚¬ê°€ ë‹¤ë¥¸ ì„¹ì…˜ì´ë‚˜ ì—¬ëŸ¬ í•­ëª©ìœ¼ë¡œ ì¤‘ë³µ ë“±ì¥í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
- **ë¶„ëŸ‰**: ê²°ê³¼ëŠ” í•œ ë‰´ìŠ¤ê¸°ì‚¬ë‹¹ **5ì¤„-8ì¤„** ìœ ì§€í•´ì£¼ê³ , ê° ì„¹ì…˜ë³„ ë¶„ëŸ‰ì„ ê· ë“±í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.
- **í˜•ì‹**: ê° ì„¹ì…˜ ì œëª©ì€ '# ê²½ì œ TOP 10'ê³¼ ê°™ì´ ë§ˆí¬ë‹¤ìš´ h1 í˜•ì‹(#)ì„ ì‚¬ìš©í•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹:
1) ì˜¤ëŠ˜ì˜ í•µì‹¬ 5ì¤„
2) # ê²½ì œ TOP 10 (ê° 6~8ì¤„, ìˆ«ìëª©ë¡, IT/ê¸°ìˆ  ë¶€ë¬¸ê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ” topic)
3) # IT/ê¸°ìˆ  TOP 10 (ê° 6~8ì¤„, ìˆ«ìëª©ë¡, ê²½ì œë¶€ë¬¸ê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ” topic)
4) ê³µí†µ íŠ¸ë Œë“œ 5~8ì¤„
5) ì¶œì²˜ ë§í¬(ì´ìŠˆë³„ ëŒ€í‘œ ë§í¬ 1ê°œì”©)
6) ê¸ˆìœµê¶Œ ê°œë°œìë¥¼ ì§€ë§í•˜ëŠ” ì·¨ì—…ì¤€ë¹„ìƒì„ ìœ„í•œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ ê²½ì œ/IT ë‰´ìŠ¤ ì „ë¬¸ ì—ë””í„°ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ê°„ê²°í•œ ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=3500,
            temperature=0.5
        )
        return response.choices[0].message.content

    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {e}"

def parse_inline_formatting(text):
    """í…ìŠ¤íŠ¸ì—ì„œ **bold**, Markdown ë§í¬, ì¼ë°˜ URL ê°™ì€ ì¸ë¼ì¸ ì„œì‹ì„ íŒŒì‹±í•˜ì—¬ Notion rich_text ê°ì²´ ë°°ì—´ë¡œ ë°˜í™˜"""
    
    rich_text_elements = []
    last_idx = 0
    
    # Regex to find all three types of formatting
    # Group 'md_text': Markdown Link Text, Group 'md_url': Markdown Link URL
    # Group 'bold_text': Bold Text
    # Group 'raw_url': Raw URL
    # Pattern order is important: Markdown link should be matched before raw URL
    pattern = re.compile(
        r'\[(?P<md_text>[^\]]+?)\]\((?P<md_url>https?:\/\/[^\s\)]+)\)'  # Markdown link
        r'|\*\*(?P<bold_text>[^\*]+?)\*\*'                               # Bold text
        r'|(?P<raw_url>https?:\/\/[^\s]+)'                                # Raw URL
    )
    
    for match in pattern.finditer(text):
        start, end = match.span()
        
        # Add preceding plain text
        if start > last_idx:
            plain_text = text[last_idx:start]
            if plain_text:
                rich_text_elements.append({"type": "text", "text": {"content": plain_text}})
        
        # Process the matched part
        if match.group('md_text'): # It's a Markdown link
            md_text = match.group('md_text')
            md_url = match.group('md_url')
            rich_text_elements.append({
                "type": "text",
                "text": {"content": md_text, "link": {"url": md_url}},
                "annotations": {"bold": False} 
            })
        elif match.group('bold_text'): # It's bold text
            bold_text = match.group('bold_text')
            rich_text_elements.append({
                "type": "text",
                "text": {"content": bold_text},
                "annotations": {"bold": True}
            })
        elif match.group('raw_url'):  # It's a raw URL
            raw_url = match.group('raw_url')

            # âœ… ë¬¸ì¥ ëì— ë¶™ëŠ” êµ¬ë‘ì /ë‹«ëŠ” ê´„í˜¸ ì œê±°
            # - ì¼ë°˜ì ìœ¼ë¡œ URLì— í¬í•¨ë˜ì§€ ì•ŠëŠ” í›„í–‰ ë¬¸ìë“¤ì„ ì œê±°
            # - í•„ìš”í•˜ë©´ ëª©ë¡ì— ë” ì¶”ê°€ ê°€ëŠ¥
            raw_url = raw_url.rstrip(').,;:!?"\'â€â€™ã€‹ã€‰ã€‘]')

            rich_text_elements.append({
                "type": "text",
                "text": {"content": raw_url, "link": {"url": raw_url}},
                "annotations": {"bold": False}
            })
            
        last_idx = end
    
    # Add any remaining plain text at the end
    if last_idx < len(text):
        plain_text = text[last_idx:]
        if plain_text:
            rich_text_elements.append({"type": "text", "text": {"content": plain_text}})
            
    return rich_text_elements

def add_to_notion(title, content, report_date_str):
    """Notion DBì— ìš”ì•½ ë¦¬í¬íŠ¸ ì¶”ê°€"""
    print("ğŸ“ Notionì— ë¦¬í¬íŠ¸ ë“±ë¡ ì¤‘...")

    api_key = os.getenv("NOTION_API_KEY")
    database_id = os.getenv("NOTION_DATABASE_ID")

    if not api_key or not database_id:
        print("âŒ Notion API í‚¤ ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— NOTION_API_KEYì™€ NOTION_DATABASE_IDë¥¼ ì„¤ì •í•˜ì„¸ìš”.\n")
        return

    try:
        notion = notion_client.Client(auth=api_key)

        children_blocks = []
        lines = content.split('\n')
        i = 0
        while i < len(lines):
            line = lines[i]
            stripped_line = line.strip()

            if not stripped_line:
                i += 1
                continue

            # --- ë¸”ë¡ ë ˆë²¨ ìš”ì†Œ ì²˜ë¦¬ ---

            # ì œëª© (Headings)
            if stripped_line.startswith('# '):
                text_content = stripped_line[2:]
                block = {"object": "block", "type": "heading_1", "heading_1": {"rich_text": parse_inline_formatting(text_content)}}
                children_blocks.append(block)
                i += 1
                continue
            elif stripped_line.startswith('## '):
                text_content = stripped_line[3:]
                block = {"object": "block", "type": "heading_2", "heading_2": {"rich_text": parse_inline_formatting(text_content)}}
                children_blocks.append(block)
                i += 1
                continue
            elif stripped_line.startswith('### '):
                text_content = stripped_line[4:]
                block = {"object": "block", "type": "heading_3", "heading_3": {"rich_text": parse_inline_formatting(text_content)}}
                children_blocks.append(block)
                i += 1
                continue

            # ëª©ë¡ ê·¸ë£¹ ì²˜ë¦¬ (Process a whole list at once)
            is_numbered = re.match(r'^\d+[\.\)]\s', stripped_line)
            is_bulleted = stripped_line.startswith(('- ', '* '))
            if is_numbered or is_bulleted:
                list_type_to_process = 'numbered' if is_numbered else 'bulleted'
                
                # Loop as long as we are in the same type of list
                while i < len(lines):
                    current_line_stripped = lines[i].strip()
                    if not current_line_stripped:
                        # empty line breaks the list
                        break 
                    
                    is_current_line_numbered = re.match(r'^\d+[\.\)]\s', current_line_stripped)
                    is_current_line_bulleted = current_line_stripped.startswith(('- ', '* '))

                    # Break if list type changes or it's not a list item
                    if (list_type_to_process == 'numbered' and not is_current_line_numbered) or \
                       (list_type_to_process == 'bulleted' and not is_current_line_bulleted) or \
                        current_line_stripped.startswith('#'):
                        break

                    # It's a valid item of the current list. Process it.
                    if is_current_line_numbered:
                        text_content = re.sub(r'^\d+[\.\)]\s', '', current_line_stripped)
                        block_type = 'numbered_list_item'
                    else:
                        text_content = current_line_stripped[2:]
                        block_type = 'bulleted_list_item'

                    # Find multi-line content for this item
                    item_content_end_index = i + 1
                    while item_content_end_index < len(lines):
                        next_line = lines[item_content_end_index]
                        next_line_stripped = next_line.strip()
                        # Stop if next line is a new list/block type or empty
                        if not next_line_stripped or next_line_stripped.startswith(('#', '- ', '* ')) or re.match(r'^\d+[\.\)]\s', next_line_stripped):
                            break
                        text_content += '\n' + next_line
                        item_content_end_index += 1

                    # Create and append the list item block
                    rich_text = parse_inline_formatting(text_content)
                    block = {"object": "block", "type": block_type, block_type: {"rich_text": rich_text}}
                    children_blocks.append(block)

                    # Move master index 'i' to the next item
                    i = item_content_end_index
                
                continue # Finished processing the list, restart main while loop

            # ì¼ë°˜ ë¬¸ë‹¨ (Paragraphs) - Fallback
            text_content = line
            i += 1
            # Consume subsequent lines until a new block starts
            while i < len(lines):
                next_line = lines[i]
                next_line_stripped = next_line.strip()
                if not next_line_stripped or \
                    next_line_stripped.startswith(('#', '- ', '* ')) or \
                    re.match(r'^\d+[\.\)]\s', next_line_stripped):
                    break
                text_content += '\n' + next_line
                i += 1
            
            block = {"object": "block", "type": "paragraph", "paragraph": {"rich_text": parse_inline_formatting(text_content)}}
            children_blocks.append(block)

        # --- Notion í˜ì´ì§€ ìƒì„± ---
        TITLE_PROPERTY_NAME = "ì´ë¦„"
        DATE_PROPERTY_NAME = "ë‚ ì§œ"

        new_page_data = {
            "parent": {"database_id": database_id},
            "properties": {
                TITLE_PROPERTY_NAME: {"title": [{"text": {"content": title}}]},
                DATE_PROPERTY_NAME: {"date": {"start": report_date_str}}
            },
            "children": children_blocks
        }

        notion.pages.create(**new_page_data)
        print("âœ… Notion ë“±ë¡ ì™„ë£Œ!\n")

    except Exception as e:
        err_msg = str(e).lower()
        if "property" in err_msg and ("does not exist" in err_msg or "unrecognized property" in err_msg):
            print(f"âŒ Notion ë“±ë¡ ì˜¤ë¥˜: ë°ì´í„°ë² ì´ìŠ¤ì— í•„ìš”í•œ ì†ì„±ì´ ì—†ê±°ë‚˜ ì´ë¦„ì´ ë‹¤ë¦…ë‹ˆë‹¤.")
            print(f"   main.py íŒŒì¼ì˜ 'add_to_notion' í•¨ìˆ˜ì—ì„œ ì†ì„± ì´ë¦„ì„ í™•ì¸í•˜ê³ ,")
            print(f"   ì‚¬ìš©ì Notion DBì˜ ì‹¤ì œ ì†ì„± ì´ë¦„ìœ¼ë¡œ TITLE_PROPERTY_NAMEê³¼ DATE_PROPERTY_NAMEì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”.")
            print(f"   (í˜„ì¬ ì„¤ì •: ì œëª©='{TITLE_PROPERTY_NAME}', ë‚ ì§œ='{DATE_PROPERTY_NAME}')\n")
        else:
            print(f"âŒ Notion ë“±ë¡ ì˜¤ë¥˜: {e}\n")


def save_report(summary, articles_count, target_date=None):
    os.makedirs("reports", exist_ok=True)
    
    if target_date:
        date_obj = datetime.strptime(target_date, "%Y-%m-%d")
        today = date_obj.strftime("%Y%m%d")
        display_date = date_obj.strftime('%Yë…„ %mì›” %dì¼')
    else:
        today = datetime.now().strftime("%Y%m%d")
        display_date = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
    
    filename = f"reports/daily_report_{today}.txt"

    report = f"""

{summary}

{"="*70}
"""
    with open(filename, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}\n")
    return filename

def main():
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='ê²½ì œ/IT ë‰´ìŠ¤ ìš”ì•½ ì„œë¹„ìŠ¤')
    parser.add_argument(
        '--date', '-d',
        type=str,
        default=None,
        help='ìˆ˜ì§‘í•  ë‚ ì§œ (YYYY-MM-DD í˜•ì‹, ì˜ˆ: 2026-01-10). ë¯¸ì§€ì •ì‹œ ì˜¤ëŠ˜'
    )
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸš€ ê²½ì œ/IT ë‰´ìŠ¤ ìš”ì•½ ì„œë¹„ìŠ¤ ì‹œì‘ (OpenAI + Naver/RSS)")
    print("="*70 + "\n")

    # âœ… ë‚ ì§œ íŒŒë¼ë¯¸í„° ì „ë‹¬
    articles = collect_news_from_naver(target_date=args.date)

    if not articles:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    articles = dedup_by_url(articles)
    unique_articles = remove_duplicates_tfidf(articles, threshold=0.72)

    summary = summarize_news(unique_articles)
    filename = save_report(summary, len(unique_articles), target_date=args.date)

    # Notionì— ë“±ë¡
    # ë‚ ì§œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì„¤ì •
    if args.date:
        report_date = datetime.strptime(args.date, "%Y-%m-%d")
    else:
        # report íŒŒì¼ ì €ì¥ì‹œ ì‚¬ìš©ëœ ë‚ ì§œì™€ ë™ì¼í•˜ê²Œ ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©
        report_date = datetime.now()

    report_title = f"{report_date.strftime('%Yë…„ %mì›” %dì¼')} ë‰´ìŠ¤ ë¸Œë¦¬í•‘"
    add_to_notion(report_title, summary, report_date.strftime("%Y-%m-%d"))

    print("="*70)
    print("âœ¨ ì™„ë£Œ! ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”:")
    print(f"   ğŸ“„ {filename}")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()
