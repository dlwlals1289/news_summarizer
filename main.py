import feedparser
import openai
import os
import time
import re
import html
import requests 
import argparse  # ì»¤ë§¨ë“œë¼ì¸ ì¸ì ì²˜ë¦¬ìš©
from urllib.parse import urlparse
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv
from dateutil import parser as date_parser
import notion_client


load_dotenv()

# í•œêµ­ ì£¼ìš” ì–¸ë¡ ì‚¬ RSS í”¼ë“œ
RSS_FEEDS = {
    "í•œêµ­ê²½ì œ": "https://www.hankyung.com/feed/economy",
    "ë§¤ì¼ê²½ì œ": "https://www.mk.co.kr/rss/30200001/",
    "ì „ìì‹ ë¬¸": "https://www.etnews.com/20/0101/list.xml",
    "ì¡°ì„ ë¹„ì¦ˆ": "https://biz.chosun.com/rss/biz_total.xml",
    "ì„œìš¸ê²½ì œ": "https://www.sedaily.com/RSS/S01.xml",
}

def _clean_html(text: str) -> str:
    """HTML íƒœê·¸/ì—”í‹°í‹° ì œê±°"""
    if not text:
        return ""
    text = html.unescape(text)
    text = re.sub(r"<[^>]+>", "", text)
    return text.strip()

def collect_news_from_rss():
    """RSSë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (fallback / ë˜ëŠ” ê¸°ë³¸)"""
    print("ğŸ“° RSSë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    all_articles = []

    for source, url in RSS_FEEDS.items():
        try:
            print(f"  â†’ {source} ìˆ˜ì§‘ ì¤‘...")
            feed = feedparser.parse(url)

            count = 0
            for entry in feed.entries[:100]:
                title = _clean_html(entry.get("title", ""))
                summary = _clean_html(entry.get("summary", ""))[:500]

                article = {
                    "title": title,
                    "link": entry.get("link", ""),
                    "published": entry.get("published", ""),
                    "summary": summary,
                    "source": source,
                    "originallink": entry.get("link", ""),  # RSSëŠ” ë³´í†µ linkê°€ ì›ë¬¸
                }
                if article["title"]:
                    all_articles.append(article)
                    count += 1

            print(f"     âœ“ {count}ê°œ ìˆ˜ì§‘")
            time.sleep(0.3)

        except Exception as e:
            print(f"     âœ— ì˜¤ë¥˜: {e}")

    print(f"\nì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!\n")
    return all_articles

def collect_news_from_naver(target_date=None):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
    
    Args:
        target_date (str): 'YYYY-MM-DD' í˜•ì‹ ë˜ëŠ” None (ì˜¤ëŠ˜)
    """
    print("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ìˆ˜ì§‘ ì‹œì‘...")
    
    # ë‚ ì§œ ì„¤ì •
    if target_date:
        try:
            target = datetime.strptime(target_date, "%Y-%m-%d")
            print(f"ğŸ“… ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: {target.strftime('%Yë…„ %mì›” %dì¼')}\n")
        except ValueError:
            print(f"âŒ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {target_date} (YYYY-MM-DD í˜•ì‹ í•„ìš”)")
            return []
    else:
        target = datetime.now()
        print(f"ğŸ“… ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: ì˜¤ëŠ˜ ({target.strftime('%Yë…„ %mì›” %dì¼')})\n")
    
    # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (í•´ë‹¹ ë‚ ì§œ 00:00 ~ 23:59)
    start_of_day = target.replace(hour=0, minute=0, second=0, microsecond=0)
    end_of_day = target.replace(hour=23, minute=59, second=59, microsecond=999999)

    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")

    if not client_id or not client_secret:
        print("âŒ ë„¤ì´ë²„ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. RSSë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n")
        return collect_news_from_rss()

    keywords = [
        "ê¸ˆìœµ", "ì¦ì‹œ", "ì£¼ì‹", "í™˜ìœ¨", "ì¦ê¶Œ", "ìºí”¼íƒˆ", 
        "IT", "AI", "í…Œí¬", "ìŠ¤í…Œì´ë¸”ì½”ì¸", "ë””ì§€í„¸ìì‚°",
        "ì‚¼ì„±ì¦ê¶Œ", "ë„¤ì´ë²„", 
        "í•˜ë‚˜ì€í–‰", "ìš°ë¦¬ì€í–‰", "ì€í–‰", "ê¸°ì—…ì€í–‰",
    ]

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }

    all_articles = []
    for keyword in keywords:
        try:
            print(f"  â†’ '{keyword}' ê²€ìƒ‰ ì¤‘...")

            params = {"query": keyword, "display": 100, "sort": "date"}
            response = requests.get(url, headers=headers, params=params, timeout=10)

            if response.status_code != 200:
                print(f"     âœ— ì˜¤ë¥˜: {response.status_code}")
                continue

            data = response.json()
            count = 0
            for item in data.get("items", []):
                # âœ… ë‚ ì§œ í•„í„°ë§ ì¶”ê°€
                pub_date_str = item.get("pubDate", "")
                if pub_date_str:
                    try:
                        # ë„¤ì´ë²„ pubDate í˜•ì‹: "Mon, 09 Jan 2026 14:30:00 +0900"
                        pub_date = date_parser.parse(pub_date_str)
                        
                        # ì§€ì •ëœ ë‚ ì§œê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ
                        if not (start_of_day <= pub_date <= end_of_day):
                            continue
                    except Exception as e:
                        # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ ì¼ë‹¨ í¬í•¨
                        pass
                
                title = _clean_html(item.get("title", ""))
                description = _clean_html(item.get("description", ""))[:500]

                originallink = item.get("originallink") or ""
                link = item.get("link") or originallink

                source_domain = "ë„¤ì´ë²„ë‰´ìŠ¤"
                try:
                    if originallink:
                        source_domain = urlparse(originallink).netloc or "ë„¤ì´ë²„ë‰´ìŠ¤"
                except Exception:
                    pass

                article = {
                    "title": title,
                    "link": link,
                    "published": pub_date_str,
                    "summary": description,
                    "source": source_domain,
                    "originallink": originallink,
                }

                if article["title"]:
                    all_articles.append(article)
                    count += 1

            print(f"     âœ“ {count}ê°œ ìˆ˜ì§‘")
            time.sleep(0.12)

        except Exception as e:
            print(f"     âœ— ì˜¤ë¥˜: {e}")

    print(f"\nì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ!\n")
    return all_articles


def dedup_by_url(articles):
    """URL ê¸°ì¤€ 1ì°¨ ì¤‘ë³µ ì œê±° (ë„¤ì´ë²„ í‚¤ì›Œë“œ ë£¨í”„ ì¤‘ë³µ ë°©ì§€)"""
    seen = set()
    out = []
    for a in articles:
        key = (a.get("originallink") or a.get("link") or "").strip()
        if not key:
            # urlì´ ì—†ìœ¼ë©´ ì œëª© ê¸°ë°˜ìœ¼ë¡œë¼ë„ í‚¤ ìƒì„±
            key = f"title::{a.get('title','')}"
        if key in seen:
            continue
        seen.add(key)
        out.append(a)
    return out

def remove_duplicates_tfidf(articles, threshold=0.72):
    """TF-IDF ê¸°ë°˜ 2ì°¨ ì¤‘ë³µ ì œê±° (title+summary)"""
    print("ğŸ” ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì¤‘...")

    if not articles:
        return []

    docs = [(a.get("title","") + " " + a.get("summary","")).strip() for a in articles]

    try:
        vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform(docs)
        sim = cosine_similarity(tfidf_matrix)

        keep = []
        removed = set()

        for i in range(len(articles)):
            if i in removed:
                continue
            keep.append(i)
            for j in range(i + 1, len(articles)):
                if sim[i][j] >= threshold:
                    removed.add(j)

        unique = [articles[i] for i in keep]
        print(f"  â†’ {len(removed)}ê°œ ì¤‘ë³µ ì œê±°")
        print(f"  â†’ {len(unique)}ê°œ ê³ ìœ  ê¸°ì‚¬ ë‚¨ìŒ\n")
        return unique

    except Exception as e:
        print(f"  âš ï¸  ì¤‘ë³µ ì œê±° ì˜¤ë¥˜: {e}")
        print(f"  â†’ ì›ë³¸ {len(articles)}ê°œ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n")
        return articles

def summarize_news(articles):
    """AI ìš”ì•½"""
    print("ğŸ¤– OpenAI GPT AI ìš”ì•½ ìƒì„± ì¤‘...\n")

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return "âŒ ì˜¤ë¥˜: OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    client = openai.OpenAI(api_key=api_key)

    # ìš”ì•½ ì…ë ¥ì„ titleë§Œ ë„£ì§€ ë§ê³  summaryë„ ê°™ì´
    # ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ë¹„ìš©/í† í° ì¦ê°€í•˜ë‹ˆ 80ê°œ ì •ë„ë¡œ ì œí•œ ê¶Œì¥
    selected = articles[:80]
    articles_text = "\n\n".join([
        f"[{a.get('source','')}] {a.get('title','')}\n- ìš”ì•½: {a.get('summary','')}\n- ë§í¬: {a.get('originallink') or a.get('link')}"
        for a in selected
    ])

    prompt = f"""ë‹¤ìŒì€ ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ í•œêµ­ ê²½ì œ/IT ë‰´ìŠ¤ ê¸°ì‚¬ {len(selected)}ê°œì…ë‹ˆë‹¤.

{articles_text}

ìš”êµ¬ì‚¬í•­:
- ê²°ê³¼ëŠ” **A4 3í˜ì´ì§€ ì´ë‚´(ì•½ 2000~2400ì)**ë¡œ ì œí•œ
- ì¤‘ë³µ ì´ìŠˆëŠ” ë°˜ë“œì‹œ í†µí•© (ê°™ì€ ì´ìŠˆ ê¸°ì‚¬ ì—¬ëŸ¬ ê°œë©´ 1ê°œë¡œ ë¬¶ì–´ì„œ)
- ì„¹ì…˜ë³„ ë¶„ëŸ‰ì„ ì§€ì¼œ ê³¼ë„í•˜ê²Œ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ
- ê° ì„¹ì…˜ ì œëª©ì€ h1ìœ¼ë¡œ

ì¶œë ¥ í˜•ì‹:
1) ì˜¤ëŠ˜ì˜ í•µì‹¬ 5ì¤„
2) ê²½ì œ TOP 10 (ê° 6~8ì¤„)
3) IT/ê¸°ìˆ  TOP 10 (ê° 6~8ì¤„)
4) ê³µí†µ íŠ¸ë Œë“œ 5~8ì¤„
5) ë‚´ì¼ ê´€ì „ í¬ì¸íŠ¸ 5ì¤„
6) ì¶œì²˜ ë§í¬(ì´ìŠˆë³„ ëŒ€í‘œ ë§í¬ 1ê°œì”©)
7) ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ ê²½ì œ/IT ë‰´ìŠ¤ ì „ë¬¸ ì—ë””í„°ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ê°„ê²°í•œ ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=3500,
            temperature=0.5
        )
        return response.choices[0].message.content

    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {e}"

def parse_inline_formatting(text):
    """í…ìŠ¤íŠ¸ì—ì„œ **bold**, Markdown ë§í¬, ì¼ë°˜ URL ê°™ì€ ì¸ë¼ì¸ ì„œì‹ì„ íŒŒì‹±í•˜ì—¬ Notion rich_text ê°ì²´ ë°°ì—´ë¡œ ë°˜í™˜"""
    
    rich_text_elements = []
    last_idx = 0
    
    # Regex to find all three types of formatting
    # Group 'md_text': Markdown Link Text, Group 'md_url': Markdown Link URL
    # Group 'bold_text': Bold Text
    # Group 'raw_url': Raw URL
    # Pattern order is important: Markdown link should be matched before raw URL
    pattern = re.compile(
        r'\[(?P<md_text>[^\]]+?)\]\((?P<md_url>https?:\/\/[^\s\)]+)\)'  # Markdown link
        r'|\*\*(?P<bold_text>[^\*]+?)\*\*'                               # Bold text
        r'|(?P<raw_url>https?:\/\/[^\s]+)'                                # Raw URL
    )
    
    for match in pattern.finditer(text):
        start, end = match.span()
        
        # Add preceding plain text
        if start > last_idx:
            plain_text = text[last_idx:start]
            if plain_text:
                rich_text_elements.append({"type": "text", "text": {"content": plain_text}})
        
        # Process the matched part
        if match.group('md_text'): # It's a Markdown link
            md_text = match.group('md_text')
            md_url = match.group('md_url')
            rich_text_elements.append({
                "type": "text",
                "text": {"content": md_text, "link": {"url": md_url}},
                "annotations": {"bold": False} 
            })
        elif match.group('bold_text'): # It's bold text
            bold_text = match.group('bold_text')
            rich_text_elements.append({
                "type": "text",
                "text": {"content": bold_text},
                "annotations": {"bold": True}
            })
        elif match.group('raw_url'):  # It's a raw URL
            raw_url = match.group('raw_url')

            # âœ… ë¬¸ì¥ ëì— ë¶™ëŠ” êµ¬ë‘ì /ë‹«ëŠ” ê´„í˜¸ ì œê±°
            # - ì¼ë°˜ì ìœ¼ë¡œ URLì— í¬í•¨ë˜ì§€ ì•ŠëŠ” í›„í–‰ ë¬¸ìë“¤ì„ ì œê±°
            # - í•„ìš”í•˜ë©´ ëª©ë¡ì— ë” ì¶”ê°€ ê°€ëŠ¥
            raw_url = raw_url.rstrip(').,;:!?"\'â€â€™ã€‹ã€‰ã€‘]')

            rich_text_elements.append({
                "type": "text",
                "text": {"content": raw_url, "link": {"url": raw_url}},
                "annotations": {"bold": False}
            })
            
        last_idx = end
    
    # Add any remaining plain text at the end
    if last_idx < len(text):
        plain_text = text[last_idx:]
        if plain_text:
            rich_text_elements.append({"type": "text", "text": {"content": plain_text}})
            
    return rich_text_elements

def add_to_notion(title, content, report_date_str):
    """Notion DBì— ìš”ì•½ ë¦¬í¬íŠ¸ ì¶”ê°€"""
    print("ğŸ“ Notionì— ë¦¬í¬íŠ¸ ë“±ë¡ ì¤‘...")

    api_key = os.getenv("NOTION_API_KEY")
    database_id = os.getenv("NOTION_DATABASE_ID")

    if not api_key or not database_id:
        print("âŒ Notion API í‚¤ ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— NOTION_API_KEYì™€ NOTION_DATABASE_IDë¥¼ ì„¤ì •í•˜ì„¸ìš”.\n")
        return

    try:
        notion = notion_client.Client(auth=api_key)

        children_blocks = []
        lines = content.split('\n')
        i = 0
        while i < len(lines):
            line = lines[i]
            stripped_line = line.strip()

            if not stripped_line:
                i += 1
                continue

            # --- ë¸”ë¡ ë ˆë²¨ ìš”ì†Œ ì²˜ë¦¬ ---

            # ì œëª© (Headings)
            if stripped_line.startswith('# '):
                text_content = stripped_line[2:]
                block = {"object": "block", "type": "heading_1", "heading_1": {"rich_text": parse_inline_formatting(text_content)}}
                children_blocks.append(block)
                i += 1
                continue
            elif stripped_line.startswith('## '):
                text_content = stripped_line[3:]
                block = {"object": "block", "type": "heading_2", "heading_2": {"rich_text": parse_inline_formatting(text_content)}}
                children_blocks.append(block)
                i += 1
                continue
            elif stripped_line.startswith('### '):
                text_content = stripped_line[4:]
                block = {"object": "block", "type": "heading_3", "heading_3": {"rich_text": parse_inline_formatting(text_content)}}
                children_blocks.append(block)
                i += 1
                continue

            # ëª©ë¡ ê·¸ë£¹ ì²˜ë¦¬ (Process a whole list at once)
            is_numbered = re.match(r'^\d+[\.\)]\s', stripped_line)
            is_bulleted = stripped_line.startswith(('- ', '* '))
            if is_numbered or is_bulleted:
                list_type_to_process = 'numbered' if is_numbered else 'bulleted'
                
                # Loop as long as we are in the same type of list
                while i < len(lines):
                    current_line_stripped = lines[i].strip()
                    if not current_line_stripped:
                        # empty line breaks the list
                        break 
                    
                    is_current_line_numbered = re.match(r'^\d+[\.\)]\s', current_line_stripped)
                    is_current_line_bulleted = current_line_stripped.startswith(('- ', '* '))

                    # Break if list type changes or it's not a list item
                    if (list_type_to_process == 'numbered' and not is_current_line_numbered) or \
                       (list_type_to_process == 'bulleted' and not is_current_line_bulleted) or \
                        current_line_stripped.startswith('#'):
                        break

                    # It's a valid item of the current list. Process it.
                    if is_current_line_numbered:
                        text_content = re.sub(r'^\d+[\.\)]\s', '', current_line_stripped)
                        block_type = 'numbered_list_item'
                    else:
                        text_content = current_line_stripped[2:]
                        block_type = 'bulleted_list_item'

                    # Find multi-line content for this item
                    item_content_end_index = i + 1
                    while item_content_end_index < len(lines):
                        next_line = lines[item_content_end_index]
                        next_line_stripped = next_line.strip()
                        # Stop if next line is a new list/block type or empty
                        if not next_line_stripped or next_line_stripped.startswith(('#', '- ', '* ')) or re.match(r'^\d+[\.\)]\s', next_line_stripped):
                            break
                        text_content += '\n' + next_line
                        item_content_end_index += 1

                    # Create and append the list item block
                    rich_text = parse_inline_formatting(text_content)
                    block = {"object": "block", "type": block_type, block_type: {"rich_text": rich_text}}
                    children_blocks.append(block)

                    # Move master index 'i' to the next item
                    i = item_content_end_index
                
                continue # Finished processing the list, restart main while loop

            # ì¼ë°˜ ë¬¸ë‹¨ (Paragraphs) - Fallback
            text_content = line
            i += 1
            # Consume subsequent lines until a new block starts
            while i < len(lines):
                next_line = lines[i]
                next_line_stripped = next_line.strip()
                if not next_line_stripped or \
                    next_line_stripped.startswith(('#', '- ', '* ')) or \
                    re.match(r'^\d+[\.\)]\s', next_line_stripped):
                    break
                text_content += '\n' + next_line
                i += 1
            
            block = {"object": "block", "type": "paragraph", "paragraph": {"rich_text": parse_inline_formatting(text_content)}}
            children_blocks.append(block)

        # --- Notion í˜ì´ì§€ ìƒì„± ---
        TITLE_PROPERTY_NAME = "ì´ë¦„"
        DATE_PROPERTY_NAME = "ë‚ ì§œ"

        new_page_data = {
            "parent": {"database_id": database_id},
            "properties": {
                TITLE_PROPERTY_NAME: {"title": [{"text": {"content": title}}]},
                DATE_PROPERTY_NAME: {"date": {"start": report_date_str}}
            },
            "children": children_blocks
        }

        notion.pages.create(**new_page_data)
        print("âœ… Notion ë“±ë¡ ì™„ë£Œ!\n")

    except Exception as e:
        err_msg = str(e).lower()
        if "property" in err_msg and ("does not exist" in err_msg or "unrecognized property" in err_msg):
            print(f"âŒ Notion ë“±ë¡ ì˜¤ë¥˜: ë°ì´í„°ë² ì´ìŠ¤ì— í•„ìš”í•œ ì†ì„±ì´ ì—†ê±°ë‚˜ ì´ë¦„ì´ ë‹¤ë¦…ë‹ˆë‹¤.")
            print(f"   main.py íŒŒì¼ì˜ 'add_to_notion' í•¨ìˆ˜ì—ì„œ ì†ì„± ì´ë¦„ì„ í™•ì¸í•˜ê³ ,")
            print(f"   ì‚¬ìš©ì Notion DBì˜ ì‹¤ì œ ì†ì„± ì´ë¦„ìœ¼ë¡œ TITLE_PROPERTY_NAMEê³¼ DATE_PROPERTY_NAMEì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”.")
            print(f"   (í˜„ì¬ ì„¤ì •: ì œëª©='{TITLE_PROPERTY_NAME}', ë‚ ì§œ='{DATE_PROPERTY_NAME}')\n")
        else:
            print(f"âŒ Notion ë“±ë¡ ì˜¤ë¥˜: {e}\n")


def save_report(summary, articles_count, target_date=None):
    os.makedirs("reports", exist_ok=True)
    
    if target_date:
        date_obj = datetime.strptime(target_date, "%Y-%m-%d")
        today = date_obj.strftime("%Y%m%d")
        display_date = date_obj.strftime('%Yë…„ %mì›” %dì¼')
    else:
        today = datetime.now().strftime("%Y%m%d")
        display_date = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
    
    filename = f"reports/daily_report_{today}.txt"

    report = f"""

{summary}

{"="*70}
"""
    with open(filename, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}\n")
    return filename

def main():
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='ê²½ì œ/IT ë‰´ìŠ¤ ìš”ì•½ ì„œë¹„ìŠ¤')
    parser.add_argument(
        '--date', '-d',
        type=str,
        default=None,
        help='ìˆ˜ì§‘í•  ë‚ ì§œ (YYYY-MM-DD í˜•ì‹, ì˜ˆ: 2026-01-10). ë¯¸ì§€ì •ì‹œ ì˜¤ëŠ˜'
    )
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸš€ ê²½ì œ/IT ë‰´ìŠ¤ ìš”ì•½ ì„œë¹„ìŠ¤ ì‹œì‘ (OpenAI + Naver/RSS)")
    print("="*70 + "\n")

    # âœ… ë‚ ì§œ íŒŒë¼ë¯¸í„° ì „ë‹¬
    articles = collect_news_from_naver(target_date=args.date)

    if not articles:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    articles = dedup_by_url(articles)
    unique_articles = remove_duplicates_tfidf(articles, threshold=0.72)

    summary = summarize_news(unique_articles)
    filename = save_report(summary, len(unique_articles), target_date=args.date)

    # Notionì— ë“±ë¡
    # ë‚ ì§œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì„¤ì •
    if args.date:
        report_date = datetime.strptime(args.date, "%Y-%m-%d")
    else:
        # report íŒŒì¼ ì €ì¥ì‹œ ì‚¬ìš©ëœ ë‚ ì§œì™€ ë™ì¼í•˜ê²Œ ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©
        report_date = datetime.now()

    report_title = f"{report_date.strftime('%Yë…„ %mì›” %dì¼')} ë‰´ìŠ¤ ë¸Œë¦¬í•‘"
    add_to_notion(report_title, summary, report_date.strftime("%Y-%m-%d"))

    print("="*70)
    print("âœ¨ ì™„ë£Œ! ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”:")
    print(f"   ğŸ“„ {filename}")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()